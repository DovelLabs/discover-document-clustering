{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rod/anaconda3/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /Users/rod/anaconda3/lib/python3.6/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: tqdm in /Users/rod/anaconda3/lib/python3.6/site-packages (from nltk) (4.31.1)\n",
      "Requirement already satisfied: click in /Users/rod/anaconda3/lib/python3.6/site-packages (from nltk) (6.7)\n",
      "Requirement already satisfied: regex in /Users/rod/anaconda3/lib/python3.6/site-packages (from nltk) (2017.4.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/Users/rod/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rod/anaconda3/lib/python3.6/site-packages (3.7.1)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /Users/rod/anaconda3/lib/python3.6/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/rod/anaconda3/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/rod/anaconda3/lib/python3.6/site-packages (from gensim) (1.18.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rod/anaconda3/lib/python3.6/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: requests in /Users/rod/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /Users/rod/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (1.12.39)\n",
      "Requirement already satisfied: boto in /Users/rod/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.48.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/rod/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rod/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/rod/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/rod/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/rod/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /Users/rod/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/rod/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/rod/anaconda3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.7.0->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/rod/anaconda3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/Users/rod/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of words (NLTK)\n",
    "We use the method word_tokenize() to split a sentence into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Mars', 'is', 'approximately', 'half', 'the', 'diameter', 'of', 'Earth', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = \"Mars is approximately half the diameter of Earth.\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of sentences (NLTK)\n",
    "An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. \n",
    "We need to count average words per sentence, so for accomplishing such a task, we use sentence tokenization as well as words to calculate the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Mars is a cold desert world ...', 'It is half the size of Earth.']\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "2"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data = \"Mars is a cold desert world ... It is half the size of Earth. \"\n",
    "tokenized_sentence = sent_tokenize(data)\n",
    "print(sent_tokenize(data))\n",
    "\n",
    "display(len(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's implement it in our similarity algorithm.\n",
    "\n",
    "Open file and tokenize sentences\n",
    "\n",
    "Create a .txt file and write 4-5 sentences in it. \n",
    "\n",
    "Include the file with the same directory of your Python program. \n",
    "\n",
    "Now, we are going to open this file with Python and split sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\Users\\\\eric.stringfellow\\\\Desktop\\\\Projects\\\\discover-document-clustering\\\\Jupyter_notebook'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Users/eric.stringfellow/Desktop/Projects/discover-document-clustering/discover-document-clustering'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0b84ee4cf420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Users/eric.stringfellow/Desktop/Projects/discover-document-clustering/discover-document-clustering'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Users/eric.stringfellow/Desktop/Projects/discover-document-clustering/discover-document-clustering'"
     ]
    }
   ],
   "source": [
    "os.chdir('Users/eric.stringfellow/Desktop/Projects/discover-document-clustering/discover-document-clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Change directory path to root directory\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(os.curdir))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\Users\\\\eric.stringfellow\\\\Desktop\\\\Projects\\\\discover-document-clustering'"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "os.path.abspath(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of documents: 5\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "with open ('data/similarity_demofile.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and create dictionary\n",
    "Once we added tokenized sentences in array, it is time to tokenize words for each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dictionary maps every word to a number. Gensim lets you read the text and update the dictionary, one line at a time, without loading the entire text file into system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'(': 0, ')': 1, ',': 2, '.': 3, 'and': 4, 'are': 5, 'associated': 6, 'customer': 7, 'even': 8, 'i.e.': 9, 'interaction': 10, 'near-real-time': 11, 'often': 12, 'operational': 13, 'or': 14, 'real-time': 15, 'service': 16, 'short-term': 17, 'situations': 18, 'that': 19, 'with': 20, 'affect': 21, 'as': 22, 'changes': 23, 'cycle': 24, 'decisions': 25, 'drive': 26, 'high-level': 27, 'in': 28, 'inflation': 29, 'interest': 30, 'investment': 31, 'longer': 32, 'more': 33, 'rates': 34, 'such': 35, 'tactical': 36, 'trends': 37, 'actions': 38, 'brexit': 39, 'by': 40, 'competitors': 41, 'e.g.': 42, 'legal': 43, 'opportunities': 44, 'regulatory': 45, 'threats': 46, 'be': 47, 'can': 48, 'disruptions': 49, 'events': 50, 'external': 51, 'frequent': 52, 'random': 53, 'rare': 54, 'weather-related': 55, 'which': 56, 'a': 57, 'aging': 58, 'cyclical': 59, 'infrastructure': 60, 'may': 61, 'of': 62, 'over': 63, 'periodic': 64, 'related': 65, 'responses': 66, 'series': 67, 'those': 68, 'time': 69, 'to': 70, 'warrant': 71}\n"
    }
   ],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a bag of words\n",
    "The next important object you need to familiarize with in order to work in gensim is the Corpus (a Bag of Words). It is a basically object that contains the word id and its frequency in each document (just lists the number of times each word occurs in the sentence).\n",
    "\n",
    "Note that, a ‘token’ typically means a ‘word’. A ‘document’ can typically refer to a ‘sentence’ or ‘paragraph’ and a ‘corpus’ is typically a ‘collection of documents as a bag of words’.\n",
    "\n",
    "Now, create a bag of words corpus and pass the tokenized list of words to the Dictionary.doc2bow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0, 1),\n (1, 1),\n (2, 3),\n (3, 1),\n (4, 1),\n (5, 1),\n (6, 1),\n (7, 2),\n (8, 1),\n (9, 1),\n (10, 1),\n (11, 1),\n (12, 1),\n (13, 1),\n (14, 1),\n (15, 1),\n (16, 1),\n (17, 1),\n (18, 1),\n (19, 1),\n (20, 1)]"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF\n",
    "Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
    "\n",
    "Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "( 0.08\n) 0.08\nand 0.14\nare 0.24\nassociated 0.24\ncustomer 0.48\neven 0.24\ni.e. 0.14\ninteraction 0.24\nnear-real-time 0.24\noften 0.24\noperational 0.24\nreal-time 0.24\nservice 0.24\nshort-term 0.24\nsituations 0.24\nthat 0.14\nwith 0.24\n( 0.07\n) 0.07\ni.e. 0.13\nthat 0.26\naffect 0.23\nas 0.03\nchanges 0.13\ncycle 0.23\ndecisions 0.46\ndrive 0.23\nhigh-level 0.23\nin 0.23\ninflation 0.23\ninterest 0.23\ninvestment 0.23\nlonger 0.23\nmore 0.23\nrates 0.23\nsuch 0.03\ntactical 0.23\ntrends 0.23\n( 0.1\n) 0.1\nand 0.35\nas 0.04\nchanges 0.17\nsuch 0.04\nactions 0.3\nbrexit 0.3\nby 0.3\ncompetitors 0.3\ne.g. 0.3\nlegal 0.3\nopportunities 0.3\nregulatory 0.3\nthreats 0.3\nas 0.05\nsuch 0.05\nbe 0.34\ncan 0.34\ndisruptions 0.34\nevents 0.19\nexternal 0.34\nfrequent 0.34\nrandom 0.34\nrare 0.34\nweather-related 0.34\nwhich 0.19\nas 0.04\nsuch 0.04\nevents 0.14\nwhich 0.14\na 0.25\naging 0.25\ncyclical 0.25\ninfrastructure 0.25\nmay 0.25\nof 0.25\nover 0.25\nperiodic 0.25\nrelated 0.25\nresponses 0.25\nseries 0.25\nthose 0.25\ntime 0.25\nto 0.25\nwarrant 0.25\n"
    }
   ],
   "source": [
    "# deconstructing the below list comprehensions\n",
    "\n",
    "# for doc in tf_idf[corpus]:\n",
    "#     for id, freq in doc:\n",
    "#         print(dictionary[id], np.around(freq, decimals= 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['(', 0.08], [')', 0.08], ['and', 0.14], ['are', 0.24], ['associated', 0.24], ['customer', 0.48], ['even', 0.24], ['i.e.', 0.14], ['interaction', 0.24], ['near-real-time', 0.24], ['often', 0.24], ['operational', 0.24], ['real-time', 0.24], ['service', 0.24], ['short-term', 0.24], ['situations', 0.24], ['that', 0.14], ['with', 0.24]]\n[['(', 0.07], [')', 0.07], ['i.e.', 0.13], ['that', 0.26], ['affect', 0.23], ['as', 0.03], ['changes', 0.13], ['cycle', 0.23], ['decisions', 0.46], ['drive', 0.23], ['high-level', 0.23], ['in', 0.23], ['inflation', 0.23], ['interest', 0.23], ['investment', 0.23], ['longer', 0.23], ['more', 0.23], ['rates', 0.23], ['such', 0.03], ['tactical', 0.23], ['trends', 0.23]]\n[['(', 0.1], [')', 0.1], ['and', 0.35], ['as', 0.04], ['changes', 0.17], ['such', 0.04], ['actions', 0.3], ['brexit', 0.3], ['by', 0.3], ['competitors', 0.3], ['e.g.', 0.3], ['legal', 0.3], ['opportunities', 0.3], ['regulatory', 0.3], ['threats', 0.3]]\n[['as', 0.05], ['such', 0.05], ['be', 0.34], ['can', 0.34], ['disruptions', 0.34], ['events', 0.19], ['external', 0.34], ['frequent', 0.34], ['random', 0.34], ['rare', 0.34], ['weather-related', 0.34], ['which', 0.19]]\n[['as', 0.04], ['such', 0.04], ['events', 0.14], ['which', 0.14], ['a', 0.25], ['aging', 0.25], ['cyclical', 0.25], ['infrastructure', 0.25], ['may', 0.25], ['of', 0.25], ['over', 0.25], ['periodic', 0.25], ['related', 0.25], ['responses', 0.25], ['series', 0.25], ['those', 0.25], ['time', 0.25], ['to', 0.25], ['warrant', 0.25]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Creating similarity measure object\n",
    "Now, we are going to create similarity object. \n",
    "\n",
    "The main class is Similarity, which builds an index for a given set of documents.\n",
    "\n",
    "The Similarity class splits the index into several smaller sub-indexes, which are disk-based. \n",
    "\n",
    "Let's just create similarity object then you will understand how we can use it for comparing.\n",
    "\n",
    "We are storing index matrix in 'workdir' directory but you can name it whatever you want and of course you have to create it with same directory of your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the index\n",
    "\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "\n",
    "index_tmpfile = get_tmpfile(\"wordindex\")\n",
    "sims = gensim.similarities.Similarity(index_tmpfile,tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Document\n",
    "Once the index is built, we are going to calculate how similar is this query document to each document in the index. So, create second .txt file which will include query documents or sentences and tokenize them as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of documents: 2\n"
    }
   ],
   "source": [
    "file2_docs = []\n",
    "\n",
    "with open ('data/similarity_demofile2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "\n",
    "#update an existing dictionary and create bag of words\n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(2, 1), (3, 1), (40, 1), (48, 1), (57, 1), (70, 2)]"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "dictionary.doc2bow(query_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarities to query \n",
    "\n",
    "At this stage, you will see similarities between the query and all index documents. \n",
    "\n",
    "To obtain similarities of our query document against the indexed documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Comparing Result: [0.         0.         0.11468977 0.12823999 0.28629014]\n"
    }
   ],
   "source": [
    "# perform a similarity query against the corpus\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "# print(document_number, document_similarity)\n",
    "# Cosine measure returns similarities in the range (the greater, the more similar).\n",
    "print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Similarity\n",
    "What's next? I think it is better to calculate average similarity of query document. \n",
    "\n",
    "At this time, we are going to import numpy to calculate sum of these similarity outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.52921987"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "np.sum(sims[query_doc_tf_idf], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.52921987\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To calculate average similarity we have to divide this value with count of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "11"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "round(float(sum_of_sims / len(file_docs))* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average similarity float: 0.10584397315979004\nAverage similarity percentage: 10.584397315979004\nAverage similarity rounded percentage: 11\n"
    }
   ],
   "source": [
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.06598131 0.09630069 0.29393423 0.04638713 0.2475149 ]\n",
      "avg: 0.15002365112304689\n",
      "Comparing Result: [0.         0.         0.11468977 0.12823999 0.28629014]\n",
      "avg: 0.10584397315979004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rod/anaconda3/lib/python3.6/site-packages/gensim/similarities/docsim.py:528: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = numpy.hstack(shard_results)\n"
     ]
    }
   ],
   "source": [
    "avg_sims = [] # array of averages\n",
    "\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "        # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # print (document_number, document_similarity)\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "        # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # print average of similarity for each query doc\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "        # add average values into array\n",
    "        avg_sims.append(avg)  \n",
    "        \n",
    "# calculate total average\n",
    "total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "total_avg = ((np.sum(avg_sims, dtype=np.float)) / len(file2_docs))\n",
    "\n",
    "# round the value and multiply by 100 to format it as percentage\n",
    "percentage_of_similarity = round(float(total_avg) * 100)\n",
    "\n",
    "# if percentage is greater than 100\n",
    "# that means documents are almost same\n",
    "if percentage_of_similarity >= 100:\n",
    "    percentage_of_similarity = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Comparing Result: [0.06598131 0.09630069 0.29393423 0.04638713 0.2475149 ]\navg: 0.15002365112304689\nComparing Result: [0.         0.         0.11468977 0.12823999 0.28629014]\navg: 0.10584397315979004\n"
    }
   ],
   "source": [
    "avg_sims = [] # array of averages\n",
    "\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "        # # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # # print (document_number, document_similarity)\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "        # # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # # print average of similarity for each query doc\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "        # # add average values into array\n",
    "        avg_sims.append(avg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "language": "python",
   "name": "python38264bitvenvvenvd98798f1f8ec4fabaeafb2b035ff7383"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}